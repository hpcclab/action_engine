{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Description</th>\n",
       "      <th>input_params</th>\n",
       "      <th>output_params</th>\n",
       "      <th>format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Username2ID</td>\n",
       "      <td>This API is to convert a username to a user ID.</td>\n",
       "      <td>{'user_name': {'description': 'the name of the...</td>\n",
       "      <td>{'user_ID': {'description': 'the ID of the use...</td>\n",
       "      <td>Username2ID(user_name) -&gt; user_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Playlistname2ID</td>\n",
       "      <td>This API is to convert a playlist name to a pl...</td>\n",
       "      <td>{'playlist_name': {'description': 'the name of...</td>\n",
       "      <td>{'playlist_ID': {'description': 'the ID of the...</td>\n",
       "      <td>Playlistname2ID(playlist_name) -&gt; playlist_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AddSongToPlaylist</td>\n",
       "      <td>This API is to add a song to a playlist.</td>\n",
       "      <td>{'user_ID': {'description': 'the ID of the use...</td>\n",
       "      <td>{'playlist_songs': {'description': 'a list of ...</td>\n",
       "      <td>AddSongToPlaylist(user_ID, playlist_ID, song_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GeoLocation2TimeZone</td>\n",
       "      <td>This API is to convert geolocation to timezone.</td>\n",
       "      <td>{'geolocation': {'description': 'the geolocati...</td>\n",
       "      <td>{'timezone': {'description': 'the timezone of ...</td>\n",
       "      <td>GeoLocation2TimeZone(geolocation) -&gt; timezone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SetAlarm</td>\n",
       "      <td>This API allows the user to set an alarm in a ...</td>\n",
       "      <td>{'timezone': {'description': 'the timezone whe...</td>\n",
       "      <td>{'alarm_status': {'description': 'the status o...</td>\n",
       "      <td>SetAlarm(timezone, time) -&gt; alarm_status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                        Description  \\\n",
       "0           Username2ID    This API is to convert a username to a user ID.   \n",
       "1       Playlistname2ID  This API is to convert a playlist name to a pl...   \n",
       "2     AddSongToPlaylist           This API is to add a song to a playlist.   \n",
       "3  GeoLocation2TimeZone    This API is to convert geolocation to timezone.   \n",
       "4              SetAlarm  This API allows the user to set an alarm in a ...   \n",
       "\n",
       "                                        input_params  \\\n",
       "0  {'user_name': {'description': 'the name of the...   \n",
       "1  {'playlist_name': {'description': 'the name of...   \n",
       "2  {'user_ID': {'description': 'the ID of the use...   \n",
       "3  {'geolocation': {'description': 'the geolocati...   \n",
       "4  {'timezone': {'description': 'the timezone whe...   \n",
       "\n",
       "                                       output_params  \\\n",
       "0  {'user_ID': {'description': 'the ID of the use...   \n",
       "1  {'playlist_ID': {'description': 'the ID of the...   \n",
       "2  {'playlist_songs': {'description': 'a list of ...   \n",
       "3  {'timezone': {'description': 'the timezone of ...   \n",
       "4  {'alarm_status': {'description': 'the status o...   \n",
       "\n",
       "                                              format  \n",
       "0                  Username2ID(user_name) -> user_ID  \n",
       "1      Playlistname2ID(playlist_name) -> playlist_ID  \n",
       "2  AddSongToPlaylist(user_ID, playlist_ID, song_n...  \n",
       "3      GeoLocation2TimeZone(geolocation) -> timezone  \n",
       "4           SetAlarm(timezone, time) -> alarm_status  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "reversechain_apis = \"/home/cc/AutomaticWorkflowGeneration/ActionEngine/db_new/api_info_new/reversechain_apis.json\"\n",
    "df = pd.read_json(reversechain_apis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chack the eval_api_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_api_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs\n",
      "Music\n",
      "Travel\n",
      "Other\n",
      "Entertainment\n",
      "Gaming\n",
      "Database\n",
      "Business_Software\n",
      "Artificial_Intelligence_Machine_Learning\n",
      "Monitoring\n",
      "SMS\n",
      "Commerce\n",
      "Health_and_Fitness\n",
      "News_Media\n",
      "Data\n",
      "Events\n",
      "Media\n",
      "Medical\n",
      "Email\n",
      "Financial\n",
      "Payments\n",
      "Communication\n",
      "Location\n",
      "Finance\n",
      "Advertising\n",
      "Cybersecurity\n",
      "Science\n",
      "Social\n",
      "Movies\n",
      "Tools\n",
      "eCommerce\n",
      "Business\n",
      "Transportation\n",
      "Weather\n",
      "Visual_Recognition\n",
      "Video_Images\n",
      "Education\n",
      "Translation\n",
      "Text_Analysis\n",
      "Mapping\n",
      "Cryptography\n",
      "Search\n",
      "Logistics\n",
      "Food\n",
      "Storage\n",
      "Sports\n",
      "Energy\n",
      "---------------------------------------------\n",
      "Number of Categories: 47\n"
     ]
    }
   ],
   "source": [
    "len(eval_api_df)\n",
    "categories = {}\n",
    "for i in range(len(eval_api_df)):\n",
    "    if eval_api_df[\"category\"][i] not in categories:\n",
    "        categories[eval_api_df[\"category\"][i]] = 1\n",
    "        print(eval_api_df[\"category\"][i])\n",
    "    else:\n",
    "        categories[eval_api_df[\"category\"][i]] += 1\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Number of Categories:\" ,len(categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cc/AutomaticWorkflowGeneration/ActionEngine/db/api_info_new/reversechain_apis.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreversechain_apis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdump(categories, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cc/AutomaticWorkflowGeneration/ActionEngine/db/api_info_new/reversechain_apis.json'"
     ]
    }
   ],
   "source": [
    "with open('./count_by_category.json', 'w') as json_file:\n",
    "    json.dump(categories, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge original funciton info and eval api info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_name                      : Random Gradient Resume Template Generator\n",
      "category                      : Jobs\n",
      "summary                       : The API generates unique resume templates with randomly generated personal, skill, and experience data using the Flask web framework and the Faker library. Users can access the main endpoint for random template generation via a GET method, which renders an HTML page displaying the created resume template. The generated resumes include a name, job title, website, skills, summary, and work experiences based on realistic information. Additionally, users can download the generated template as a ZIP file through the download endpoint. Both endpoints have no required or optional parameters. The output of the main endpoint is an HTML page displaying the random resume template, while the download endpoint generates a ZIP file containing the template and its assets.\n",
      "name                          : nan\n",
      "Number of Categories: 179\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "Id                            : 1\n",
      "name                          : tti_Animation_Art\n",
      "input_parameters_with_datatype: [{'name': 'prompt', 'datatype': 'string', 'description': \"The 'prompt' parameter is a user prompt in string data type. It is required for providing descriptive content or themes for the image generation process in the 'tti_Animation_Art' API. This parameter serves as the textual input that will be transformed into vibrant animation-style images by the API's pre-trained model.\"}]\n",
      "input_description             : The input parameter for this API is a user prompt in string data type.\n",
      "output_data_type              : binary_image_file\n",
      "output description            : Generated animation image as a binary_image_file\n",
      "description                   : This API takes a user prompt and generates an image in animation style using a pre-trained model. The user needs to provide a prompt in string format and the API will return the generated animation image as a response in the form of a FileResponse.\n",
      "method                        : POST\n",
      "url                           : nan\n",
      "summary                       : The 'tti_Animation_Art' API utilizes a POST method to transform user prompts into captivating animation images. By inputting a user prompt in string format, individuals can provide descriptive content or themes for the image generation process. The API employs a sophisticated pre-trained model to convert these textual inputs into vibrant animation-style images. Upon processing the input prompt, the API seamlessly delivers the resulting artwork as a binary_image_file, encapsulating the dynamically generated animation. This API empowers users to effortlessly merge textual creativity with visual artistry, enabling the creation of detailed animated images for various digital media applications and creative projects.\n",
      "source_code                   : from fastapi import FastAPI, HTTPException\n",
      "from fastapi.responses import FileResponse\n",
      "\n",
      "from PIL import Image\n",
      "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
      "import torch\n",
      "from pydantic import BaseModel\n",
      "from pathlib import Path\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "def load_model():\n",
      "    model_id = 'dreamlike-art/dreamlike-anime-1.0'\n",
      "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
      "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
      "    pipe = pipe.to('cuda')\n",
      "    return pipe\n",
      "\n",
      "def process_data(prompt, pipe):\n",
      "    response = pipe(prompt).images[0]\n",
      "    return response\n",
      "\n",
      "@app.post(\"/generate_anime_image/\")\n",
      "async def generate_image(prompt:str):\n",
      "    try:\n",
      "\n",
      "        # Load the model (replace load_model() with your actual model loading logic)\n",
      "        pipe = load_model()\n",
      "\n",
      "        # Prompt Creation\n",
      "        prompt = \"Anime-style picture: \" + prompt\n",
      "\n",
      "        # Process the data (replace process_data() with your actual data processing logic)\n",
      "        response = process_data(prompt, pipe)\n",
      "\n",
      "        ## Save the image\n",
      "        response.save('generated_img.png')\n",
      "\n",
      "        image_path = Path('generated_img.png')\n",
      "        \n",
      "        if not image_path.is_file():\n",
      "            return {\"error\": \"Image not found on the server\"}\n",
      "\n",
      "        return FileResponse(image_path)\n",
      "    except Exception as e:\n",
      "        # Handle exceptions or errors here\n",
      "        raise HTTPException(status_code=500, detail=str(e))\n",
      "Number of Categories: 15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check\n",
    "\"\"\"\n",
    "for c in eval_api_df.columns:\n",
    "\n",
    "    print(f'{c:<30}: {eval_api_df.loc[0, c]}')\n",
    "print(\"Number of Categories:\" ,len(eval_api_df))\n",
    "print('+'*40)\n",
    "\n",
    "for c in api_df.columns:\n",
    "\n",
    "    print(f'{c:<30}: {api_df.loc[0, c]}')\n",
    "print(\"Number of Categories:\" ,len(api_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items:  194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13200/2252715759.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_api_df_selected.rename(columns={'api_name': 'name'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Marge the 2 dataframes: eval_api_df(179) + api_df(15) => combined_df (194)\n",
    "\"\"\"\n",
    "\n",
    "api_df[\"category\"] = \"Image Generation\"\n",
    "# Step 1: Select and rename columns in eval_api_df to match api_df\n",
    "eval_api_df_selected = eval_api_df[['api_name', 'category', 'summary']]\n",
    "eval_api_df_selected.rename(columns={'api_name': 'name'}, inplace=True)\n",
    "\n",
    "# Step 2: Add missing columns to eval_api_df_selected (fill with NaN or default values)\n",
    "columns_to_add = [col for col in api_df.columns if col not in eval_api_df_selected.columns]\n",
    "for col in columns_to_add:\n",
    "    eval_api_df_selected[col] = None  # or fill with a default value\n",
    "\n",
    "# Step 3: Align columns and concatenate vertically\n",
    "eval_api_df_selected = eval_api_df_selected[api_df.columns]  # Ensure column order matches\n",
    "df = pd.concat([api_df, eval_api_df_selected], axis=0, ignore_index=True)\n",
    "df['Id'] = range(1, len(df) + 1)\n",
    "\n",
    "df = df.filter(items=['Id','name', 'category', 'summary'])\n",
    "\n",
    "print(\"Number of items: \",len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'name', 'category', 'summary'], dtype='object')\n",
      "['Image Generation' 'Jobs' 'Music' 'Travel' 'Other' 'Entertainment'\n",
      " 'Gaming' 'Database' 'Business_Software'\n",
      " 'Artificial_Intelligence_Machine_Learning' 'Monitoring' 'SMS' 'Commerce'\n",
      " 'Health_and_Fitness' 'News_Media' 'Data' 'Events' 'Media' 'Medical'\n",
      " 'Email' 'Financial' 'Payments' 'Communication' 'Location' 'Finance'\n",
      " 'Advertising' 'Cybersecurity' 'Science' 'Social' 'Movies' 'Tools'\n",
      " 'eCommerce' 'Business' 'Transportation' 'Weather' 'Visual_Recognition'\n",
      " 'Video_Images' 'Education' 'Translation' 'Text_Analysis' 'Mapping'\n",
      " 'Cryptography' 'Search' 'Logistics' 'Food' 'Storage' 'Sports' 'Energy']\n",
      "48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>190</td>\n",
       "      <td>Face Detection with Age and Gender Estimation</td>\n",
       "      <td>Video_Images</td>\n",
       "      <td>The Face Detection with Age and Gender Estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>191</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Travel</td>\n",
       "      <td>The Hotels API provides comprehensive informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>192</td>\n",
       "      <td>Call of Duty</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>The Call of Duty API provides access to stats ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>193</td>\n",
       "      <td>Funny Joke Dataset</td>\n",
       "      <td>Database</td>\n",
       "      <td>The Funny Joke Dataset API provides a collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>194</td>\n",
       "      <td>BasketAPI</td>\n",
       "      <td>Sports</td>\n",
       "      <td>BasketAPI provides comprehensive basketball da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id                                           name      category  \\\n",
       "189  190  Face Detection with Age and Gender Estimation  Video_Images   \n",
       "190  191                                         Hotels        Travel   \n",
       "191  192                                   Call of Duty        Gaming   \n",
       "192  193                             Funny Joke Dataset      Database   \n",
       "193  194                                      BasketAPI        Sports   \n",
       "\n",
       "                                               summary  \n",
       "189  The Face Detection with Age and Gender Estimat...  \n",
       "190  The Hotels API provides comprehensive informat...  \n",
       "191  The Call of Duty API provides access to stats ...  \n",
       "192  The Funny Joke Dataset API provides a collecti...  \n",
       "193  BasketAPI provides comprehensive basketball da...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check\n",
    "\"\"\"\n",
    "print(df.columns)\n",
    "category_counts = df['category'].unique()\n",
    "print(category_counts)\n",
    "print(len(category_counts))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide by Number of APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of row counts where you want to split the DataFrame\n",
    "row_splits = [20, 40, 60, 80, 100, 120, 140, 160, 180, 194]\n",
    "\n",
    "# Create empty list to store the resulting DataFrames\n",
    "dfs_numbers = []\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Loop over the row splits to create separate DataFrames\n",
    "for end_index in row_splits:\n",
    "    # Select the rows for the current split\n",
    "    split_df = df.iloc[:end_index]\n",
    "    dfs_numbers.append(split_df)\n",
    "    \n",
    "    # Update the start index for the next split\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide by Number of Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'Image Generation', 'Jobs', 'Music', 'Travel', 'Other', 'Entertainment',\n",
    "    'Gaming', 'Database', 'Business_Software',\n",
    "    'Artificial_Intelligence_Machine_Learning', 'Monitoring', 'SMS', 'Commerce',\n",
    "    'Health_and_Fitness', 'News_Media', 'Data', 'Events', 'Media', 'Medical',\n",
    "    'Email', 'Financial', 'Payments', 'Communication', 'Location', 'Finance',\n",
    "    'Advertising', 'Cybersecurity', 'Science', 'Social', 'Movies', 'Tools',\n",
    "    'eCommerce', 'Business', 'Transportation', 'Weather', 'Visual_Recognition',\n",
    "    'Video_Images', 'Education', 'Translation', 'Text_Analysis', 'Mapping',\n",
    "    'Cryptography', 'Search', 'Logistics', 'Food', 'Storage', 'Sports', 'Energy'\n",
    "]\n",
    "\n",
    "# List of row counts where you want to split the DataFrame\n",
    "row_splits = [5, 10,15, 20, 25, 30, 35, 40, 48]\n",
    "\n",
    "# Create empty list to store the resulting DataFrames\n",
    "dfs_domain = []\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Loop over the row splits to create separate DataFrames\n",
    "for end_index in row_splits:\n",
    "    # Select the rows for the current split\n",
    "    categories_list = categories[:end_index]\n",
    "    split_df = df[df['category'].isin(categories_list)]\n",
    "    dfs_domain.append(split_df)\n",
    "    \n",
    "    # Update the start index for the next split\n",
    "    start_index = end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Split 9\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Split\", len(dfs_domain))\n",
    "print(len(dfs_domain[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make differrent Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'name', 'category', 'summary'], dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Databese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken -->  0.027750015258789062\n",
      "Time Taken -->  0.055271148681640625\n",
      "Time Taken -->  0.06352424621582031\n",
      "Time Taken -->  0.07059979438781738\n",
      "Time Taken -->  0.08420610427856445\n",
      "Time Taken -->  0.11052083969116211\n",
      "Time Taken -->  0.13030004501342773\n",
      "Time Taken -->  0.1194465160369873\n",
      "Time Taken -->  0.1652538776397705\n",
      "Time Taken -->  0.16869497299194336\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate vector database for different number of APIs \n",
    "[20, 40, 60, 80, 100, 120, 140, 160, 180, 194]\n",
    "\"\"\"\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for i in range(len(dfs_numbers)):\n",
    "    passage_data = dfs_numbers[i]\n",
    "    start=time.time()\n",
    "    metadatas = []\n",
    "    for index, row in passage_data.iterrows():\n",
    "        doc_meta = {\n",
    "            \"id\": row['Id'],\n",
    "            \"name\": row['name'],\n",
    "            \"category\": row['category']\n",
    "        }\n",
    "        metadatas.append(doc_meta)\n",
    "\n",
    "    faiss = FAISS.from_texts(passage_data['summary'].tolist(), embedding_function, metadatas)\n",
    "    print(\"Time Taken --> \", time.time()-start) \n",
    "    num = len(passage_data)\n",
    "    faiss.save_local(path + 'vectordb/LangChain_FAISS/eval_numbers', f\"eval_api_vec_num_{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken -->  0.032562971115112305\n",
      "Time Taken -->  0.04890608787536621\n",
      "Time Taken -->  0.0625\n",
      "Time Taken -->  0.08231306076049805\n",
      "Time Taken -->  0.0986180305480957\n",
      "Time Taken -->  0.11283135414123535\n",
      "Time Taken -->  0.11978411674499512\n",
      "Time Taken -->  0.12639284133911133\n",
      "Time Taken -->  0.14655709266662598\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate vector database for different number of APIs \n",
    "[5, 10,15, 20, 25, 30, 35, 40, 48]\n",
    "\"\"\"\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "split = [5, 10, 15, 20, 25, 30, 35, 40, 48]\n",
    "for i in range(len(dfs_domain)):\n",
    "    passage_data = dfs_domain[i]\n",
    "    start=time.time()\n",
    "    metadatas = []\n",
    "    for index, row in passage_data.iterrows():\n",
    "        doc_meta = {\n",
    "            \"id\": row['Id'],\n",
    "            \"name\": row['name'],\n",
    "            \"category\": row['category']\n",
    "        }\n",
    "        metadatas.append(doc_meta)\n",
    "\n",
    "    faiss = FAISS.from_texts(passage_data['summary'].tolist(), embedding_function, metadatas)\n",
    "    print(\"Time Taken --> \", time.time()-start) \n",
    "    num = str(split[i])\n",
    "    faiss.save_local(path + 'vectordb/LangChain_FAISS/eval_domains', f\"eval_api_vec_domain_{num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_faiss = FAISS.load_local(path + 'vectordb/LangChain_FAISS/', embedding_function, \"api_vec\", allow_dangerous_deserialization=True)\n",
    "k = 16\n",
    "start=time.time()\n",
    "Top_k = loaded_faiss.similarity_search_with_score(\"Animated image Generation?\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mTop_k\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Top_k[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(Top_k))\n",
    "Top_k[15][0].metadata[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
